class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=256, n_head=4, dropout=0.1, relative_positional_distance=100):
        super().__init__()
        self.d_model = d_model
        self.n_head = n_head
        self.d_qkv = d_model // n_head
        self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
        self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
        self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))
        self.w_o = nn.Parameter(torch.Tensor(n_head, d_qkv, d_model))
        nn.init.xavier_normal_(self.w_q)
        nn.init.xavier_normal_(self.w_k)
        nn.init.xavier_normal_(self.w_v)
        nn.init.xavier_normal_(self.w_o)
        self.dropout = nn.Dropout(dropout)
        self.relative_positional = LearnedRelativePositionalEmbedding(relative_positional_distance, n_head, d_qkv, True)

    def forward(self, x):
        q = torch.einsum("tbf,hfa->bhta", x, self.w_q)
        k = torch.einsum("tbf,hfa->bhta", x, self.w_k)
        v = torch.einsum("tbf,hfa->bhta", x, self.w_v)
        logits = torch.einsum("bhqa,bhka->bhqk", q, k) / (self.d_qkv**0.5)
        q_pos = q.permute(2, 0, 1, 3)
        l, b, h, d = q_pos.size()
        position_logits, _ = self.relative_positional(q_pos.reshape(l, b * h, d))
        logits = logits + position_logits.view(b, h, l, l)
        probs = self.dropout(F.softmax(logits, dim=-1))
        o = torch.einsum("bhqk,bhka->bhqa", probs, v)
        out = torch.einsum("bhta,haf->tbf", o, self.w_o)
        return out
