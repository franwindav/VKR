\newpage
\subsection{Transformer}

В теоретической части была дана общая информация о трансформере. В данном же разделе будет описана подробная информация о работе данного типа нейронной сети.

Классический трансформер состоит из двух основных частей, называемых энкодером и декодером \cite{bib:atten:2}. Однако в работе будет использоваться только первая часть данной архитектуры, которая изображена в левой части рисунка \ref{transformer}, а реализация на рисунке \ref{code:transformer}.

\addimgh{transformer}{0.8}{Архитектура трансформера}{transformer}

\input{code/transformer}

Как уже говорилось ранее важной особенностью transformer является то, что он не использует слова последовательно, а использует всю последовательность сразу. Это оказывает значительное влияние на время обучения, поскольку допускает параллельную обработку, что приводит к существенному увеличению скорости, однако проблема заключается в том, что порядок слов плохо воспринимается (уровнем внимания) модели. Для решения этой проблемы необходимо ввести информацию о порядке слов во входные данные, которые будут переданы Transformer (на картинке данная процедура обозначается как «positional encoding»).

\subsubsection{Механизм внимания}

Вообще говоря, внимание описывает способность модели обращать внимание на важные части предложения (или изображения, или любого другого последовательного ввода). Оно делает это путем присвоения весов входным признакам на основе их важности и положения в последовательности.

Оно также играет ключевую роль в расширении контекста, который языковая модель способна учитывать при обработке и генерации языка. Это позволяет модели создавать контекстуально соответствующие и связные тексты в гораздо более длинных последовательностях.

Внимание во многом похоже на задачу поиска, когда при задании запроса $q$, мы хотим найти набор ключей $k$, наиболее похожих на $q$, и вернуть соответствующие значения $v$.

Чтобы получить вектор-строки $q$, $k$ и $v$, представление каждого элемента входной последовательности умножается на обучаемые матрицы $W_q$ , $W_k$ и $W_v$.

Близость запроса к ключу определяется с помощью скалярного произведения:
\begin{equation}
    \text{weights}_i = \text{softmax}\biggl(\dfrac{q_ik_1^T}{C},\dfrac{q_ik_2^T}{C},\dots\biggr)
\end{equation}
где $C$ – некоторая нормировочная константа. Все вышеописанные действия отображены на рисунке \ref{self_attention}.

\addimgh{self_attention}{0.32}{Порядок операций self attention}{self_attention}

Один набор $Q$, $W$ и $K$ может передавать лишь определённые типы взаимосвязей между токенами, ограничивая информацию, получаемую из входных представлений. Для преодоления этого ограничения, принято использовать механизм "multi-head attention", в котором несколько параллельных "голов" внима\-ния с различными весами применяются для агрегации разнообразных аспектов информации. Принцип работы данного механизма изображен на рисунке \ref{multi_attention}, а реализация показана на рисунке \ref{code:attention}.

\addimgh{multi_attention}{0.47}{Принцип работы multi-head attention}{multi_attention}

\input{code/attention}

\subsubsection{Языковая модель}

В качестве алгоритма вывода CTC будет использоваться метод поиска луча. Для его реализации была выбрана 5-граммная языковая модель, исходя из их основного достоинства, которое заключается в простоте имплементации и высокой скорости работы алгоритма.

5-грамма – это статистическая модель, которая позволяет выбирать наиболее подходящее слово по контексту на основе вероятности сочетания последовательности слов.

Для расчёта вероятности предложения используется следующее правило:
\begin{equation}
    P(A,B) = P(B|A)P(A).
\end{equation}

Для 5 слов в предложении формула будет выглядеть следующим образом:
\begin{equation}
    P(A_1,A_2,A_3,A_4,A_5) = P(A_5|A_4,A_3,A_2,A_1)\cdot\ldots\cdot P(A_2|A_1)\cdot P(A_1).
\end{equation}

Таким образом, возможно оценить общую вероятность всей цепочки, ум\-ножив условные вероятности. Однако довольно сложно точно определить вероятность слова при условии длинной последовательности предшествующих слов из-за большого числа возможных последовательностей и возможного отсутствия этих выражений в наших данных. Поэтому наилучшим вариантом является приблизительное вычисление вероятностей. А для этого используются цепи Маркова, которые позволяют предсказывать вероятность элемента последовательности, не учитывая слишком широкий контекст.

В общем виде марковская цепь k-ого порядка (когда мы учитываем контекст только последних k слов) будет выглядеть так:
\begin{equation}
    P(w_i|w_1,w_2,\ldots,w_{i-1}) \approx P(w_i|w_{i-k},w_{i-k+1},\ldots,w_{i-1})
\end{equation}
В данном случае будет использоваться марковская цепь 5-ого порядка.

Чем длиннее последовательность, тем более детализированной становится наша модель, то есть более длинные предложения содержат больше грамматики по сравнению с короткими. Одновременно с этим, чем длиннее последовательность, тем реже нам встречаются случаи употребления, что означает, что многие наблюдения возникают лишь один раз.
