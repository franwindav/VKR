\subsection{Классификация тихой речи}

Распознавание речи с использованием ЭМГ имеет долгую историю исследований. Путем анализа и моделирования аудиосигналов удается преобразовать речевое содержание в текст для разнообразных приложений \cite{bib:speech:1}. Тем не менее, возникают ситуации, когда звуковые сигналы нельзя получить четко.

Распознавание беззвучной речи устраняет ограничения автоматического распознавания звука, когда акустические сигналы не доступны. Однако распознавание тихой речи сложнее обычного.

Идея использовать ЭМГ для распознавания речи существует давно. Некоторые из первых исследований в области ЭМГ и речи были осуществлены Эдфельдтом в 1959 году при изучении субвокальной речи. В начале 2000-х годов люди начали применять системы автоматического распознавания речи, основанные на скрытых моделях Маркова, к речи на основе ЭМГ, включая тихую и субвокальную речь.

Алгоритмы классификации играют важную роль в распознавании беззвучной речи с использованием ЭМГ. В текущее время наиболее популярным подходом в распознавании речи является End2End.

\subsubsection{End2end подход}

С начала 2014 года увеличился интерес исследований к полному распознаванию речи. Ранее использовались традиционные методы, основанные на фонетике, требующие отдельных компонентов и обучения произношению, акустической и языковой модели \cite{bib:end-to-end:1}. Новые комплексные модели изучают все аспекты распознавания речи совместно, оптимизируя критерии для улучшения качества \cite{bib:end-to-end:2}. Это значительно упрощает процесс обучения и внедрения этих систем.

Проблема заключается в том, что речь — это непрерывная последовательность звуков, но на выходе требуется получить дискретную последовательность. До 2006 года не было эффективного способа моделировать этот процесс. Требовалась сложная разметка для каждой записи, указывающая момент произнесения каждого звука или буквы. Эта трудоемкая задача препятствовала проведению множества исследований в этой области. Однако в 2006 году статья Алекса Грейвса "Connectionist temporal classification" (CTC) предложила решение данной проблемы, хотя вычислительных ресурсов не хватало на тот момент для его широкого применения. Реальные алгоритмы распознавания речи стали доступны значительно позже.

\subsubsection{CTC алгоритм}

Тринадцать лет назад был представлен алгоритм CTC как средство для обучения акустических моделей без необходимости сложной пофреймовой разметки, то есть без точного соответствия между фреймами входной и выходной последовательностей \cite{bib:CTC:1}. Следует отметить, что контекстно-зависимые фонемы, основанные на CTC, достигают высоких результатов в распознавании свободной речи.

Пофреймовое выравнивание подразумевает установление соответствия между фреймами сигнала и транскрипции, пример выравнивания показан на рисунке \ref{signal_aligment}. Энкодер принимает акустические признаки и выдает скрытое состояние, на основе которого с помощью функции softmax вычисляются условные вероятности. Обычно энкодер представлен слоями LSTM или другими типами RNN, также возможно использование энкодера архитектуры transformer \cite{bib:CTC:2}.

\addimgh{signal_aligment}{1}{пофреймовое выравнивание}{signal_aligment}

Для выравнивания последовательностей в общий словарь вводится специальный токен («пробел»). Следовательно, вероятности символов, выводимые нейронной сетью, также включают вероятности наличия пустого символа для каждого временного шага.

Важно понимать отличие обычного пробела и специального пробела. Пробел – это реальный символ, который используется при написании текстов. Специальный пробел же означает отсутствие какого-либо символа и используется для обозначения границ между символами.

Основной задачей CTC алгоритма является максимизация вероятности предсказания правильной последовательности символов, благодаря обобщению возможных вариантов выравнивания.

CTC предлагает несколько преимуществ в задачах обучения от последовательности к последовательности, в том числе:

\vspace{0.5em-\topsep}
\begin{itemize}
    \item[1)] Упрощенный процесс обучения: CTC устраняет необходимость в явном согласовании на уровне фрейма входных данных и выходных последовательностей, делая процесс обучения более простым и эффективным;
    \item[2)] Сквозное обучение: CTC обеспечивает сквозное обучение моделей, уменьшая необходимость в разработке сложных объектов и нескольких этапах обработки;
    \item[3)] Гибкость: CTC может применяться к различным задачам последовательного обучения, таким как распознавание речи, текста и даже жестов.
\end{itemize}

\subsubsection{Режим CTC loss}

На вход алгоритма CTC поступает последовательность $y$. Данная последовательность представляет собой матрицу, в столбцах которой находятся вектора содержащие вероятности символов из алфавита $A'$ в момент времени $t$. Тогда обозначим $y_k^t$  как вероятность символа $k$ в момент времени $t$. На рисунке \ref{loss_matrix} представлен пример матрицы вероятностной $y$.

Введем понятие пути в данной матрицы. Итак, путь ($\pi$) – это конечная последовательность, в которой каждый элемент соответствует символу из алфавита $A'$. Символы в данном последовательности закодированы номерами от $1$ до $n$, из-за чего можно сделать обратное преобразование (декодирование) для получения последовательности символов.

\addimgh{CTC_loss_matrix}{1}{матрица вероятностей}{loss_matrix}

Рассмотрим различные пути в предсказанной трансформером матрице. С помощью следующей формуле можно вычислить вероятность каждого путя при заданном $x$:
\begin{equation}
    p(\pi | x) = \prod\limits_{t=1}^T y^t_{\pi_t},\hspace{0.5cm} \forall\pi\in {A'}^T 
\end{equation}

Распознавание речи основывается на том, что каждая произнесенная буква должна быть соотнесена с соответствующим символом, а полученные последовательности необходимо разделить одиночным пробелом. Однако записанные сигналы, соответствующие одному символу, могут быть произнесены на период времени, более долгий чем выбранный временной промежуток. Из-за чего в матрице предсказаний одному символу, ожидаемому в выходной последовательности, может соответствовать несколько столбцов.

Ключом к алгоритму CTC является использование специального токена, часто называемого пустым токеном. Это просто еще один токен, который модель будет предсказывать, и он является частью словаря.

Стоит отметить важную особенность алфавита. Для корректной работы алгоритма используется специальный символ, который также предсказывает нейронная сеть. Данный токен необходим для распознавания слов с повторяющимися буквами.

Для корректной интерпретации сигналов ЭМГ необходимо сделать ряд действий. Сначала необходимо соединить все повторяющиеся символы в пути, после чего следует удаление специальных токенов, входящих в алфавит $A'$.

Для обучения нейронной сети с помощью метода обратного распространения ошибки необходима функция, которая сможет вычислять ошибку предсказания для входной последовательности $x$ и ожидаемого результата $l$. Помимо этого необходимо, чтобы эта функция была дифференцируема по переменным выходного слоя нейронной сети $y_k^t$. Определим такую функцию ошибки как минус натуральный логарифм от вероятности верного результата:
\begin{equation}
    L_{\text{CTC}} = -\ln\bigl(p(l|x)\bigr)
\end{equation}

Для получения вероятности верного результата $l$ необходимо сложить вероятности всех путей:
\begin{equation}
    p(l|x) = \sum\limits_{\pi\in\mathbb{N}^T:\text{B}(\pi)=l} p(\pi|x)
\end{equation}

Так как нахождение всех путей, ведущих к результату $l$ с помощью функции B перебором всех возможных путей очень затратно, необходим более оптимальный способ. Именно поэтому алгоритм CTC основан на алгоритме динамического программирования прямого-обратного хода.

Прежде чем рассматривать построение данного алгоритма, необходимо ввести некоторые обозначения. Формула $q_{1:p}$  означает, что из последовательности $q$ выделили подпоследовательность, состоящую из первых ее $p$ символов.  Также, определим последовательность $l'$, полученную из $l$ путем вставки символа пропуска между любыми двумя символами $l$, а также в начале и в конце $l$.

На примере рисунка \ref{loss_way} разберем как строятся пути. Необходимо отметить, что для $π$ необходимо выполнение следующего условия
\begin{equation}
    \text{B}(\pi)=l
\end{equation}

Для определения вероятностей всех путей, выделенных в таблице, которые начинаются с первой или второй строки первого столбца, а заканчиваются на пересечении строки $s$ и столбца $t$, необходимо определить функцию  $\alpha_t(s)$ следующим образом:
\begin{equation}
    \alpha_t(s) = \sum\limits_{\pi\in\mathbb{N}^T:\text{D}(\pi_{1:t})\in\bigl\{l_{1:s}',l_{2:s}'\bigr\}}\prod\limits_{\tau=1}^t y_{\pi_{\tau}}^\tau
\end{equation}
где $\text{D}$ -- преобразование, склеивающее одинаковые символы, стоящие последовательно, в один. 

\addimgh{CTC_loss_way}{0.8}{построение путей}{loss_way}

Для вычисления суммы вероятностей всех путей, с учетом того, что путь может заканчиваться либо специальным токеном, либо последним символом из $l$, нужно вычислить сумму функций альфа на последнем шаге времени от последнего символа $l$ и от символа пропуска:
\begin{equation}
    p(l|x) = \alpha_T(\bigl[l'\bigr]-1)+\alpha_T(\bigl[l'\bigr])
\end{equation}

Теперь можно применять эту формулу для вычисления функции потерь CTC. Однако для обучения нейронной сети методом обратного распространения ошибки необходимо знать, как вычислять частную производную от функции $L_{\text{CTC}}$ по переменным выходного слоя $y_k^t$:
\begin{equation}
    \dfrac{\partial L_{\text{CTC}}}{\partial y_k^t} = -\dfrac{1}{p(l|x)}\dfrac{\partial p(l|x)}{\partial y_k^t}.
\end{equation}

Для этого необходимо определить функцию, которая сможет вычислять вероятность всех путей, начинающихся в ячейке $(s,t)$ таблицы и заканчивающихся на последнем символе или на символе пропуска, следующим образом:
\begin{equation}
    \beta_t(s) = \sum\limits_{\pi\in\mathbb{N}:\text{D}(\pi_{1:T})\in\bigl\{l_{s:[l']}',l_{s:[l']-1}'\bigr\}}\prod\limits_{\tau=1}^t y_{\pi_{\tau}}^\tau.
\end{equation}

Рассмотрим значения $\beta_t(s)$ для последнего столбца.
\begin{equation}
    \beta_T([l']) = y_b^T 
\end{equation}
\begin{equation}
    \beta_T([l']-1) = y^T
\end{equation}
\begin{equation}
    \beta_T(s) = 0, \forall < [l'] - 1 
\end{equation}

Также как и с функцией $\alpha_t(s)$, для $\beta_t(s)$ существуют рекуррентные соотношения, вытекающие из определения функции $\text{B}$, позволяющие вычислить эту функцию для всех ячеек таблицы 2.3:
\begin{equation}
    \beta_t(s) = 
    \begin{cases}
        \bigl(\beta_{t+1}(s) + \beta_{t+1}(s+1)\bigr)y_{l'_s}^t, &\text{если } l_s'=b \text{ или } l_{s+2}'=l_s'\\
        \bigl(\beta_{t+1}(s) + \beta_{t+1}(s+1) + \beta_{t+1}(s+2)\bigr)y_{l'_s}^t, &\text{иначе}\\
    \end{cases}
\end{equation}

Заметим, что при данном $l$ выражение $\alpha_t(s)\beta_t(s)$ в конкретной ячейке $(s,t)$ дает суммарную вероятность всех путей, проходящих на шаге времени $t$ через символ $s$:
\begin{equation}
    \alpha_t(s)\beta_t(s) = \sum\limits_{\pi\in\mathbb{N}^T:\text{B}(\pi)=l,\pi_t=l_s'} y_{l_s'}^t \prod\limits_{\tau=1}^T y_{\pi_{\tau}}^\tau.
\end{equation}
Отсюда получаем что:
\begin{equation}
    \dfrac{\alpha_t(s)\beta_t(s)}{y_{l_s'}^t} = \sum\limits_{\pi\in\mathbb{N}^T:\text{B}(\pi)=l,\pi_t=l_s'} \prod\limits_{\tau=1}^T y_{\pi_{\tau}}^\tau = \sum\limits_{\pi\in\mathbb{N}:\text{B}(\pi)=l,\pi_t=l_s'} p(\pi|x).
\end{equation}
Тогда для каждого шага времени $t$ мы можем вычислить
\begin{equation}
    p(l|x) = \sum\limits_{s=1}^{[l']}\dfrac{\alpha_t(s)\beta_t(s)}{y_{l_s'}^t}.
\end{equation}
Возьмем частную производную от этого выражения по переменной выхода нейронной сети, соответствующей символу $k$ на шаге времени $t$:
\begin{equation}
    \dfrac{\partial p(l|x)}{\partial y_k^t}=-\dfrac{1}{\bigl(y_k^t\bigr)^2}\sum\limits_{s:l_s'=k}\alpha_t(s)\beta_t(s).
\end{equation}
И подставляя, получаем искомую частную производную для функции $L_{CTC}$:
\begin{equation}
    \dfrac{\partial L_\text{CTC}}{\partial y_k^t} = \dfrac{1}{\alpha_T([l']-1)+\alpha_T([l'])}\dfrac{1}{\bigl(y_k^t\bigr)^2}\sum\limits_{s:l_s'=k}\alpha_t(s)\beta_t(s).
\end{equation}

\subsubsection{Режим CTC decode}

Для реализции режима CTC decode существует два алгоритма: максимальное декодирование (greedy search) и лучевой поиск (beam search). Рассмотрим их более подробно.

\paragraph{Алгоритм максимального декодирования}

Самым вероятным результатом исходя из предсказанной матрицы, изображенной на рисунке \ref{loss_matrix}, будет являться
\begin{equation}
    h(x) = \underset{l\in A,\ l\leq T}{\text{arg max }}p(l|x)
\end{equation}

Но так как число возможных результатов $l$ для перебора слишком высоко мы будем использовать приближенные решения. Очевидным решением является выбрать путь собранный из элементов с максимальной вероятностью на каждом шаге $t$ в матрице предсказаний:
\begin{equation}
    h(x) \approx \text{B}(\underset{\pi\in\mathbb{N}^T}{\text{arg max }}p(\pi|x))
\end{equation}
Иллюстрацию этого решения можно увидеть на рисунке \ref{greedy_search}.

\vspace{0.5em}
\addimgh{greedy_search}{0.9}{решение алгоритмом greedy search}{greedy_search}

Но такое решение не всегда дает наиболее вероятный результат из-за того, что одному и тому же результату может соответствовать несколько путей и вероятность какого-то результата может оказаться больше, чем вероятность очевидного решения.

\paragraph{Алгоритм лучевого поиска}

Декодирование поиска луча - это процесс, где алгоритм последовательно генерирует и оценивает различные куски данных (размером с луч), чтобы найти оптимальное решение. Основным параметром в этом методе является ширина луча, которая определяет количество символов, сохраняемых для анализа различных вариантов.

Рассмотрим данный алгоритм. На каждом шаге вычисляются две вероятности: $P_b(t,l)$ и $P_{nb}(t,l)$. $P_b$  показывает вероятность того, что любой путь, который может быть преобразован операцией $B$ в последовательность $l$, заканчивается специальным токеном. В свою очередь функция $P_{nb}$  высчитывает вероятность того, что аналогичная последовательность заканчивается символом из алфавита $A$.

Также для использования алгоритма необходимо ввести пару вспомогательных функций: $\text{num}(c)$ и $\text{last}(s)$. Первая из них необходима для кодирования символа в его числовое значение, вторая же должна выдавать последний элемент конечной последовательности $s$.

Входные данные состоят из матрицы вероятностей, полученной из трансформера, обозначим элементы матрицы следующим образом
\begin{equation}
    P(t, \text{num}(c)),
\end{equation}
где $t$ -- номер столбца,\\ \phantom{где} $c$ -- символ алфавита $A'$.

Также в данном алгоритме используется функция $P_{LM}(x)$, позволяющая вычислять вероятности последовательностей слов в речи.

Вначале алгоритма необходимо задать начальные значения (условия):
\begin{equation}
    \begin{array}{c}
        P_b(0, \varnothing) = 1,\\ 
        P_nb(0,\varnothing) = 0,\\ 
        R_0 = \varnothing,
    \end{array}
\end{equation}
где $R_t$ -- множество лучших последовательностей на шаге $t$.


После инициализации начинается основная часть алгоритма, которая заключается в переборе символов $c$ из алфавита $A'$. На каждом шаге алгоритма происходит следующая проверка:

Если c является специальным токеном, который обозначим символом «\_», то вероятность $P_b(t,l)$ изменяется следующим образом:
\begin{equation}
    P_b(t,l)\mathrel{{+}{=}} P(t,\text{num}(\_))\bigl(P_b(t-1,l)+P_{nb}(t-1,l)\bigr)
\end{equation}
В противном случае возможны 3 ситуации:
\begin{itemize}
    \item[1.] $c = \text{last}(l)$, т.е. дублируется прошлый символ.

        Если символ является специальным токеном, тогда вероятность изменяется следующим образом
        \begin{equation}
            P_{nb}(t,l+c)\mathrel{{+}{=}} P(t,\text{num}(c))P_b(t-1,l)
        \end{equation}
        иначе
        \begin{equation}
            P_{nb}(t,l)\mathrel{{+}{=}}P(t,\text{num}(c))P_{nb}(t-1,l).
        \end{equation}
    \item[2.] Новый символ является пробелом или достигнут конец временной последовательности.

        Происходит оценка текущей ситуации последовательности слов в последовательности с помощью языковой модели:
        \begin{equation}
            P_{nb}(t,l+c)\mathrel{{+}{=}}P(t,\text{num}(c))\bigl(P_{nb}(t-1,l)+P_b(t-1,l)\bigr)P_{LM}(l+c).
        \end{equation}
    \item[3.] Если не выполнены условия предыдущих ситуаций.

        Задаем вероятность текущей последовательности с помощью формулы
        \begin{equation}
            P_{nb}(t,l+c)\mathrel{{+}{=}}P(t,\text{num}(c))\bigl(P_{nb}(t-1,l)+P_b(t-1,l)\bigr).
        \end{equation}
\end{itemize}

После того, как были вычислены все вероятности последовательностей, происходит их сортировка и выбор $k$ последовательностей $R_t$ с наибольшей вероятностью. В данном алгоритме число $k$ является шириной луча.

После завершения всех итераций берется наиболее вероятная последовательность, являющаяся результатом декодирования. На рисунке \ref{beam_search} показаны первые шаги данного алгоритма.

\addimg{beam_search}{1}{пример работы алгоритма beam search}{beam_search}

\subsubsection{Рекурентные нейронные сети}

Рекуррентные нейронные сети, или RNN, это типы нейронных сетей, специализирующиеся на обработке последовательных данных, особенно в задачах обработки естественного языка.

Основное отличие RNN от традиционных нейронных сетей заключается в их способности использовать информацию из предыдущих временных шагов для воздействия на текущие входные и выходные данные.

Еще одним важным аспектом RNN является совместное использование параметров на различных уровнях сети, в отличие от прямых нейронных сетей, где каждый узел имеет свои собственные веса.

Далее рассмотрим основные виды нейронных сетей.

\begin{itemize}[parsep=0.4em]
    \setlength\itemsep{0.8em plus 0.2em minus 0.2em}
    \item[1.] Двунаправленные рекуррентные нейронные сети (BRNN)

        BRNN представляет собой отличный вид нейронной сети, основанный на архитектуре RNN. В отличие от однонаправленных RNN, которые опираются только на предыдущие входные данные для прогнозирования текущего состояния, двунаправленные RNN используют как предыдущие, так и будущие данные, чтобы повысить точность своих прогнозов.

    \item[2.] Долговременная кратковременная память (LSTM)

        LSTM, представленные Зеппом Хохрайтером и Юргеном Шмидхубером, и являющиеся известной архитектурой RNN, были разработаны для решения проблемы исчезающего градиента \cite{bib:LSTM:1}. Эти LSTM включают в себя специальные "ячейки" в скрытых слоях нейронной сети, состоящие из трех ключевых элементов: элемента ввода, элемента вывода и элемента забывания. Эти элементы контролируют поток информации, необходимый для точного прогнозирования выходных данных в сети.
    
    \item[3.] Управляемые рекуррентные блоки (GRU)

        Этот вариант RNN напоминает LSTM в своем подходе к преодолению проблемы кратковременной памяти моделей RNN. Вместо использования информации о "состоянии ячейки", он оперирует с скрытыми состояниями и имеет два вентиля — вентиль сброса и вентиль обновления, вместо трех, как у LSTM. Аналогично шлюзам в LSTM, эти вентили определяют, какую информацию и в каком объеме сохранить.
\end{itemize}

\subsubsection{Нейронная сеть transformer}

Transformer – это нейронные сети, которые изучают контекст и понимание посредством последовательного анализа данных. Данная архитектура была представлена компанией Google за 2017 год как одна из самых передовых моделей, когда-либо разработанных. В моделях Transformer используется современный и развивающийся набор математических методов, обычно известный как внимание или самонаблюдение. Этот набор помогает определить, как удаленные элементы данных влияют друг на друга и зависят друг от друга.

Трансформаторы черпают вдохновение из архитектуры кодировщика декодера, используемой в RNNS, из-за их механизма внимания \cite{bib:atten:1}. Transformer, в отличие от RNN, не выполняет обработку данных в последовательном порядке, что обеспечивает большее распараллеливание и более быстрое обучение.

Подход к обработке последовательностей целиком через внимание позволяет избавиться от такого понятия, как скрытое состояние, обновляющееся рекуррентно: каждый токен может напрямую «прочитать» любую часть последовательности, наиболее полезную для предсказания. В частности, отсутствие рекуррентности означает, что мы можем применять слой ко всей последовательности одновременно, так как матричные умножения прекрасно параллелятся.

\subsubsection{Языковые модели}

Языковая модель представляет собой вероятностное распределение по последовательностям слов. Она оценивает вероятность для каждой последовательности слов определенной длины m. Рассмотрим основыне типы языковых моделей.

\begin{itemize}[parsep=0.4em]
    \setlength\itemsep{0.8em plus 0.2em minus 0.2em}
    \item[1.] Статистические языковые модели
    
        Статистические языковые модели – это тип моделей, которые используют статистические закономерности в данных для прогнозирования вероятности определенных последовательностей слов \cite{bib:LM:1}. Основным подходом к построению вероятностной языковой модели является вычисление n - граммовых вероятностей.

        N-граммы представляют собой последовательность слов определенной длины, где число "n" больше нуля. Для построения базовой вероятностной языковой модели вычисляются вероятности различных n-граммов (групп слов) в тексте путем подсчета их встречаемости и деления на количество появлений предыдущего слова. Этот подход основан на принципе Маркова, согласно которому вероятность следующего слова зависит только от предшествующего, игнорируя более дальний контекст. Хоть n-граммы просты и эффективны, они ограничены в учете долгосрочных зависимостей между словами в последовательности.

    \item[2.] Нейронные языковые модели

        Нейронные языковые модели, использующие нейронные сети для прогнозирования последовательностей слов, обучаются на обширных текстовых наборах данных, позволяя им изучать основы языковой структуры. Эти модели способны работать с обширными словарями и обрабатывать редкие или неизвестные слова, используя векторные представления. В области обработки естественного языка (NLP) часто применяются архитектуры нейронных сетей, такие как рекуррентные нейронные сети (RNN) и трансформеры.

        Нейронные языковые модели обладают более высокой способностью анализировать контекст по сравнению с традиционными статистическими моделями. Кроме того, они способны обрабатывать сложные языковые структуры и учитывать долгосрочные зависимости между словами.

\end{itemize}

\subsubsection{Метрики оценки точности}

Вопрос оценки эффективности систем распознавания речи является актуальным как для разработчиков собственных решений, так и для конечных пользователей. Первым шагом, который необходимо предпринять, - определить критерий оценки, то есть что именно будет измеряться, например, точность распознавания речи, скорость обработки, устойчивость к шуму в источнике данных. Затем следует выбрать показатель (метрику), который определит конкретное свойство в рамках выбранного критерия, например, процент правильно распознанных слов, время обработки аудиофрагмента, максимальный уровень шума и т.д.

Рассмотрим наиболее популярные метрики для оценки точности распознавания речи.

\begin{itemize}[parsep=0.4em]
    \setlength\itemsep{0.8em plus 0.2em minus 0.2em}
    \item[1.] Word Error Rate (WER)

        Метрика WER представляет собой процент ошибок в словах, который измеряется по количеству операций замены, удаления и вставки слов при преобразовании одной строки в другую. Чем ближе значение метрики WER к нулю, тем выше точность распознавания, поскольку это указывает на меньшее количество ошибок в распознаваемых словах.

    \item[2.] Relative Information Lost (RIL)
    
    Относительная потеря информации (RIL) — это показатель, который отражает степень информации, содержащейся в целевой фразе, и сохраняющейся в полученной моделью транскрипции. Он вычисляется как отношение условной энтропии целевой фразы к энтропии целевой фразы в полученном транскрипте.

    \item[3.] Charater Error Rate (CER)

    CER – это метрика, аналогичная WER, но она измеряет процент ошибок не в словах, а в отдельных символах.
\end{itemize}

Имеются и другие редкие методы оценки качества, учитывающие различные типы операций с переменной значимостью. Например, замена оценивается более легко, чем вставка или удаление. Также имеются алгоритмы, учитывающие разнообразие слов и букв с различными весами, что применяется при оценке эффективности систем автоматического перевода.
